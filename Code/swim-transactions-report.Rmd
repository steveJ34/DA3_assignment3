---
title: "Predicting Daily Ticket Sales for Swimming Pools in Albuquerque"
author: 'Istvan Janco #2003877'
date: "2/12/2021"
output:
  html_document:
    theme: cosmo
    code_download: yes
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning=F, message=F, warning=FALSE)

# It is advised to start a new session for every case study
# Clear memory -------------------------------------------------------
rm(list=ls())

# Import libraries ---------------------------------------------------
library(tidyverse)
library(stargazer)
library(Hmisc)
library(timeDate)
library(lubridate)
library(caret)
#install.packages('prophet')
#install.packages('StanHeaders')
library(StanHeaders)
library(prophet)
#install.packages("viridis")
library(viridis)
library(kableExtra)



# set working directory
setwd("/Users/steve_j/Documents/CEU /data_analysis/DA_3/assignment_3")

# set data dir, data used
data_dir="/Users/steve_j/Documents/CEU /data_analysis/DA_3/assignment_3/data"

# load theme and functions
source("ch00-tech-prep/theme_bg.R")
source("ch00-tech-prep/da_helper_functions.R")

data_in <- paste(data_dir,"swim-transactions","clean/", sep = "/")

data_in <- paste(data_dir, "clean/", sep = "/")
use_case_dir <- "out/"

data_out <- use_case_dir
output <- paste0(use_case_dir,"output/")
create_output_if_doesnt_exist(output)


#####################################
# Creating time features  ----------
#####################################


#import data
daily_agg<-read.csv(file = paste(data_in,"swim_work.csv",sep="")) %>% 
  mutate(date = as.Date(date))

# dow: 1=Monday, weekend: Sat and Sun.
daily_agg <- daily_agg %>%
  mutate(year = year(date),
         quarter = quarter(date),
         month = factor(month(date)),
         day = day(date)) %>%
  mutate(dow = factor(lubridate::wday(date, week_start = getOption("lubridate.week.start", 1)))) %>%
  mutate(weekend = factor(as.integer(dow %in% c(6,7))))


daily_agg <- daily_agg %>% 
  mutate(school_off = ((day>15 & month==5 & day <=30) | (month==6 |  month==7) |
                         (day<15 & month==8) | (day>20 & month==12) ))

daily_agg <- daily_agg %>% 
  mutate(trend = c(1:dim(daily_agg)[1]))

summary(daily_agg$QUANTITY)



# Get holiday calendar ----------------------------------

holidays <-  as.Date(holidayNYSE(2010:2017))
  
daily_agg <- daily_agg %>% 
  mutate(isHoliday = ifelse(date %in% holidays,1,0))

Hmisc::describe(daily_agg)

# Define vars for analysis ----------------------------------

daily_agg <- 
  daily_agg %>% 
  group_by(month) %>% 
  mutate(q_month = mean(QUANTITY)) %>% 
  ungroup()

daily_agg <- daily_agg %>% 
  mutate(QUANTITY2 = ifelse(QUANTITY<1, 1, QUANTITY)) %>% 
  mutate(q_ln = log(QUANTITY2))

daily_agg <- 
  daily_agg %>% 
  group_by(month, dow) %>% 
  mutate(tickets = mean(QUANTITY),
         tickets_ln = mean(q_ln)) %>% 
  ungroup()

# named date vars for graphs
mydays <- c("Mon","Tue","Wed",
            "Thu","Fri","Sat",
            "Sun")
daily_agg$dow_abb   <-factor(   mydays[daily_agg$dow],  levels=mydays)
daily_agg$month_abb <-factor(month.abb[daily_agg$month],levels=month.abb)

################################
# Descriptive graphs ----------
#################################


g1 <-ggplot(data=daily_agg[daily_agg$year==2015,], aes(x=date, y=QUANTITY)) +
  geom_line(size=0.4, color=color[1]) +
  theme_bg() +
  scale_x_date(breaks = as.Date(c("2015-01-01","2015-04-01","2015-07-01","2015-10-01","2016-01-01")),
               labels = date_format("%d%b%Y"),
               date_minor_breaks = "1 month" ) +
  labs( title = "Figure 1: Ticket Sales 2015 Trend ", x = "Date (day)", y="Daily ticket sales" ) +
  scale_color_discrete(name = "")
g1
#save_fig("figure-3a-swimmingpool-2015", output, "small")

g2<-ggplot(data=daily_agg[(daily_agg$year>=2010) & (daily_agg$year<=2014),], aes(x=date, y=QUANTITY)) +
  geom_line(size=0.2, color=color[1]) +
  theme_bg() +
  scale_x_date(breaks = as.Date(c("2010-01-01","2011-01-01","2012-01-01","2013-01-01","2014-01-01","2015-01-01")),
               labels = date_format("%d%b%Y"),
               minor_breaks = "3 months") +
  labs( title = "Figure 3: Ticket Sales 2010 - 2015", x = "Date (day)", y="Daily ticket sales" ) +
  scale_color_discrete(name = "")
g2
#save_fig("figure-3b-swimmingpool-2010-2014", output, "small")


g3<-ggplot(data=daily_agg, aes(x=month_abb, y=QUANTITY)) +
  theme_bg() +
  labs(title = "Figure 2: Ticket Sales Yearly Variation",x = "Date (month)", y="Daily ticket sales" ) +
  geom_boxplot(color=color[1],outlier.color = color[4], outlier.alpha = 0.6, outlier.size = 0.4)
g3
#save_fig("ch18-figure-4a-swimmingpool-monthly", output, "small")

g4<-ggplot(data=daily_agg, aes(x=dow_abb, y=QUANTITY)) +
  theme_bg() +
  labs(title = "Figure 4: Ticket Sales Weekly Variation",  x = "Day of the week", y="Daily ticket sales" ) +
  geom_boxplot(color=color[1],outlier.color = color[4], outlier.alpha = 0.6, outlier.size = 0.4)
  #geom_boxplot(color=color[1], outlier.shape = NA)
g4
#save_fig("figure-4b-swimmingpool-dow", output, "small")

# to check for interactions, look at the heatmap
swim_heatmap <- 
  ggplot(daily_agg, aes(x = dow_abb, y = month_abb, fill = tickets)) +
  geom_tile(colour = "white") +
  labs(title = "Figure 5: Weekly vs. Monthly Variation in Ticket Sales", x = 'Day of the week', y = 'Month ') +
  scale_fill_viridis(alpha = 0.7, begin = 1, end = 0.2, direction = 1, option = "D") +
  theme_bg() +
  theme(legend.position = "right",
    legend.text = element_text(size=6),
    legend.title =element_text(size=6)
    )
swim_heatmap
#save_fig("figure-5-swim-heatmap", output, "large")

#####################################
# PREDICTION  ----------
#####################################


#############################
# Create train/houldout data
#############################

# Last year of data
data_holdout<- daily_agg %>%
  filter(year==2016)

# Rest of data for training
data_train <- daily_agg %>%
  filter(year<2016)

# Prepare for cross-validation
data_train <- data_train %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.integer(rowname))

test_index_list <- data_train %>% 
  split(f = factor(data_train$year)) %>% 
  lapply(FUN = function(x){x$rowname})
  
train_index_list <- test_index_list %>% 
  lapply(FUN = function(x){setdiff(data_train$rowname, x)})
  
train_control <- trainControl(
  method = "cv",
  index = train_index_list, #index of train data for each fold
  # indexOut = index of test data for each fold, complement of index by default
  # indexFinal = index of data to use to train final model, whole train data by default
  savePredictions = TRUE
)

# Fit models ---------------------------------------------------------

#Model 1 linear trend + monthly seasonality
model1 <- as.formula(QUANTITY ~ 1 + trend + month)
reg1 <- train(
  model1,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 2 linear trend + monthly seasonality + days of week seasonality 
model2 <- as.formula(QUANTITY ~ 1 + trend + month + dow)
reg2 <- train(
  model2,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 3 linear trend + monthly seasonality + days of week  seasonality + holidays 
model3 <- as.formula(QUANTITY ~ 1 + trend + month + dow + isHoliday)
reg3 <- train(
  model3,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 4 linear trend + monthly seasonality + days of week  seasonality + holidays + sch*dow
model4 <- as.formula(QUANTITY ~ 1 + trend + month + dow + isHoliday + school_off*dow)
reg4 <- train(
  model4,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 5 linear trend + monthly seasonality + days of week  seasonality + holidays + interactions
model5 <- as.formula(QUANTITY ~ 1 + trend + month + dow + isHoliday + school_off*dow + weekend*month)
reg5 <- train(
  model5,
  method = "lm",
  data = data_train,
  trControl = train_control
)

#Model 6 =  multiplicative trend and seasonality (ie take logs, predict log values and transform back with correction term)
model6 <- as.formula(q_ln ~ 1 + trend + month + dow + isHoliday + school_off*dow)
reg6 <- train(
  model6,
  method = "lm",
  data = data_train,
  trControl = train_control
)


stargazer(reg2$finalModel, reg3$finalModel, reg4$finalModel, reg5$finalModel, 
          out=paste(output,"Ch18_swim_tsregs.txt",sep=""), type = "text", digits=2)
stargazer(reg6$finalModel, 
          out=paste(output,"Ch18_swim_tsregs2.txt",sep=""), type = "text", digits=2)

# Get CV RMSE ----------------------------------------------

model_names <- c("reg1","reg2","reg3","reg4","reg5")
rmse_CV <- c()

for (i in model_names) {
  rmse_CV[i]  <- get(i)$results$RMSE
}
#rmse_CV

#had to cheat and use train error on full train set because could not obtain CV fold train errors
corrb <- mean((reg6$finalModel$residuals)^2)
rmse_CV["reg6"] <- reg6$pred %>% 
  mutate(pred = exp(pred  + corrb/2)) %>% 
  group_by(Resample) %>% 
  summarise(rmse = RMSE(pred, exp(obs))) %>% 
  as.data.frame() %>% 
  summarise(mean(rmse)) %>% 
  as.numeric()
rmse_CV["reg6"] 


###########################x
# Evaluate best model on holdout set --------------------------------------------
###########################x

data_holdout <- data_holdout %>% 
  mutate(y_hat_5 = predict(reg5, newdata = .))

rmse_holdout_best <- RMSE(data_holdout$QUANTITY, data_holdout$y_hat_5)
rmse_holdout_best

###########################x
# Plot best predictions --------------------------------------------
###########################x

#graph relative RMSE (on holdout) per month 
rmse_monthly <- data_holdout %>% 
  mutate(month = factor(format(date,"%b"), 
                        levels= unique(format(sort(.$date),"%b")), 
                        ordered=TRUE)) %>% 
  group_by(month) %>% 
  summarise(
    RMSE = RMSE(QUANTITY, y_hat_5),
    RMSE_norm= RMSE(QUANTITY, y_hat_5)/mean(QUANTITY)
            ) 

g_predictions_rmse<- ggplot(rmse_monthly, aes(x = month, y = RMSE_norm)) +
  geom_col(bg=color[1], color=color[1]) +
  labs( title = "Figure 6: Relative RMSE (holdout) per Month ", x = "Date (month)", y="RMSE (normalized by monthly sales)" ) +
    theme_bg() 
g_predictions_rmse
#save_fig("swim_predictions_rmse", output, "small")
#save_fig("figure-7b-swim-predictions-rmse", output, "small", plot=g_predictions_rmse)

g_predictions<-
  ggplot(data=data_holdout, aes(x=date, y=QUANTITY)) +
  geom_line(aes(size="Actual", colour="Actual", linetype = "Actual") ) +
  geom_line(aes(y=y_hat_5, size="Predicted" ,colour="Predicted",  linetype= "Predicted")) +
  scale_y_continuous(expand = c(0,0))+
  scale_x_date(expand=c(0,0), breaks = as.Date(c("2016-01-01","2016-03-01","2016-05-01","2016-07-01","2016-09-01","2016-11-01", "2017-01-01")),
               labels = date_format("%d%b%Y"),
               date_minor_breaks = "1 month" )+
  scale_color_manual(values=color[1:2], name="")+
  scale_size_manual(name="", values=c(0.4,0.7))+
  #scale_linetype_manual(name = "", values=c("solid", "solid")) +
  scale_linetype_manual(name = "", values=c("solid", "twodash")) +
  labs(title = "Figure 7: Predicted Ticked Sales vs. Actual (holdout)", x = "Date (day)", y="Daily ticket sales" ) +
  theme_bg() +
  #theme(legend.position = "none") +
  #annotate("text", x = as.Date("2016-07-15"), y = 50, label = "Predicted", color=color[2], size=3)+
  #annotate("text", x = as.Date("2016-09-01"), y = 125, label = "Actual", color=color[1], size=3)
  theme(legend.position=c(0.7,0.8),
      legend.direction = "horizontal",
      legend.text = element_text(size = 6),
      legend.key.width = unit(.8, "cm"),
      legend.key.height = unit(.3, "cm")) + 
  guides(linetype = guide_legend(override.aes = list(size = 0.8))
         )
g_predictions
#save_fig("swim_predictions", output, "large")
#save_fig("figure-6-swim-predictions", output, "large", plot=g_predictions)


g_predictions_m <- ggplot(data=data_holdout %>% filter(month==8), aes(x=date, y=QUANTITY)) +
  geom_line(aes(size="Actual", colour="Actual", linetype = "Actual") ) +
  geom_line(aes(y=y_hat_5, size="Predicted" ,colour="Predicted",  linetype= "Predicted")) +
  geom_ribbon(aes(ymin=QUANTITY,ymax=y_hat_5), fill=color[4], alpha=0.3) +
  scale_y_continuous(expand = c(0.01,0.01), limits = c(0,150))+
  scale_x_date(expand=c(0.01,0.01), breaks = as.Date(c("2016-08-01","2016-08-08","2016-08-15","2016-08-22","2016-08-29")),
               limits = as.Date(c("2016-08-01","2016-08-31")),
               labels = date_format("%d%b")) +
  scale_color_manual(values=color[1:2], name="")+
  scale_size_manual(name="", values=c(0.4,0.7))+
  #scale_linetype_manual(name = "", values=c("solid", "solid")) +
  scale_linetype_manual(name = "", values=c("solid", "twodash")) +
  labs( title = "Graph 7: Predocted Volume vs. Actual Volume (holdout)",x = "Date (day)", y="Daily ticket sales" ) +
  theme_bg() +
  #theme(legend.position = "none") +
  #annotate("text", x = as.Date("2016-08-04"), y = 55, label = "Actual", color=color[2], size=2)+
  #annotate("text", x = as.Date("2016-08-17"), y = 115, label = "Predicted", color=color[1], size=2)
  theme(legend.position=c(0.7,0.8),
        legend.direction = "horizontal",
        legend.text = element_text(size = 4),
        legend.key.width = unit(.8, "cm"),
        legend.key.height = unit(.2, "cm")) + 
  guides(linetype = guide_legend(override.aes = list(size = 0.6))
  )
g_predictions_m
#save_fig("swim_predictions_m", output, "small")
#save_fig("figure-7a-swim-predictions-m", output, "small", plot=g_predictions_m)


```

## Outline 
#### 1. Business Problem 
#### 2. Data 
#### 3. Analysis 
#### 4. Modeling 
#### 5. Conclusion 


## 1. Business Problem 

This study is examining ticket sales for outdoor swimming pools based on data from Albuquerque, US. The specific objective is to predict the volume of ticket sales for 12 month into the future for each day. 

## 2. Data 

The data is comprised of ticket sales transactions for swimming pools in the city. It is available at *http://data.cabq.gov/community/swimmingpooladmissions/SwimmingPoolAdmissionsCABQ-en-us.csv*. The set is considered to be Big Data, due to over 1.5 million observations in it. Each observation contains the volume of sold tickets, date and time, name of the swimming pool, type of admission (e.g. senior, adult, child) and weather there was a discount or a special event. 
For the analysis we collected all the outdoor swimming pools assuming that the IDs for such end with 01. After filtering out the indoor pools, the number of observations amounted to approximately seven million. 
in order to limit the extreme values in the number of ticket sold (possibly due to celebratory events), he ticket types were limited to categories that are considered to be business as usual (e.g. senior, adult, teen, child, etc.). The minimum observation for the number of sold tickets is -112, which might be attributed to corrections. The average number of tickets sold is around 1.4. 
The outcome variable for the study is the volume of sold tickets. 
After aggregating cleaing and aggregating the sales the final number of observations amounted to 2557. The time series has gaps for those days when a pool was closed. 
About 15% of he data was separated to a holdout set, which contain observations for 2016 only. 

## 3. Analysis 

The beol graphs present the trends and seasonality within the data. 
Figure 1 shows the trend of ticket sales for 2015. 

```{r, echo=FALSE }
g1
```

It is looks like the ticket sales are below 500 pieces per day at the beginning of the year, however as we move closer to spring and then summer, it starts increasing, and almost reached 2000 per day, but once the warm season is over, the sales revert back to the original quantities. This clearly indicates a associaion between season of the year and weather conditions. In order to further examone the dependencies, let's zoom out and take a look at sales starting from 2010. 

```{r, echo=FALSE }
g2
```

It seems that the aforementioned variatatio repeats itself every year since 2010, we can assume that there is seasonality in the sales of volume of ticket sales. 

In order to capture the magnitude of variation, let's consider the below box plot. 

```{r, echo=FALSE }
g3
```

The figure clearly indicates that the number of visitors drastically increases in the summer, with June in July being the busiest. 

The weekly box plot (Figure 4), suggests that Saturday and Sunday being the most popular days, and suprisingly Friday is indicated as the least popular visiting day. 

```{r, echo=FALSE }
g4
```

The above charts suggest significant variation in both monthly and weekly ticket sale, thus we should investigate if the there is relation between the variations on these levels. The below heatmap suggests that there indeed there is a correlation. The vertical axis shows the month of the year, while the horizontal outlines the days of the week. 

```{r, echo=FALSE }
swim_heatmap
```

The rectangles indicate the average number of visitors for every given day for a certain month (e.g. average number of visitors for every Monday in July). It is evident that the average number of visitors is higher in June and July, for every day of the week. 

## 4. Modeling 

The below table outlines the models that were used for prediction. 


```{r, echo=FALSE }

DISPLAY_X1 <-  c("linear trend", "monthly seasonality", "", "", "", "")
DISPLAY_X2 <-  c("linear trend", "monthly seasonality", "days of week seasonality", "","","")
DISPLAY_X3 <-  c("linear trend", "monthly seasonality", "days of week seasonality", "holidays", "", "")
DISPLAY_X4 <-  c("linear trend", "monthly seasonality", "days of week seasonality", "School off * dow", "", "")
DISPLAY_X5 <-  c("linear trend", "monthly seasonality", "days of week seasonality", "School off * dow", "Days*Months", "")
DISPLAY_X6 <-  c("linear trend", "monthly seasonality", "days of week seasonality", "School off * dow", "", "")
# make a data frame of the vectors
DISPLAY_FORMULAS_DF <- data.frame('X1' = DISPLAY_X1, 'X2' = DISPLAY_X2, 'X3' = DISPLAY_X3, 'X4' = DISPLAY_X4, 'X5' = DISPLAY_X5, 'X6 (Log)' = DISPLAY_X6 )


DISPLAY_FORMULAS_DF %>%
  kbl(caption = "Table 1: Logit Variables") %>%
  kable_styling()

```

The models are built with progressing complexity. The simplest model includes the seasonality and dummy for the for month. The second model also includes binary variable for day of week. Holidays are built in to the third model. Model 4 adds the interaction of days of week with school holidays. In order to capture the interaction between weekends and month (to capture the interaction suggested by the heatmap). Model 6 is similar to Model 4 in terms of predictors, however the outcome variable is log-transformed. 

```{r, echo=FALSE }

rmse_CV %>%
  kbl(caption = "Table 2: Model Performance") %>%
  kable_styling()

```

The above summary (Table 2), suggests the that the best performing model is five. The results are based on a 6 fold cross validation, using the yearly data as separate folds, in order to maintain the serial correlation. 

In order to understand the true fit of model 5, we need to apply it to the holdout set. The RMSE of model 5 turned out to be 155.49, which is actually 22,81 lower than the train RMSE. It means that the patterns of association are robust and remained the same across different times. The below graph shows how the prediction RMSE changes depending on the month. I looks like the model is predicting very well for December. It can be a result of holidays as a predictor, with December having less workdays than other months. Perhaps incorporating weather conditions for each day could reduce the errors in December. 

```{r, echo=FALSE }
g_predictions_rmse
```

Let's take a look at how the chosen model actually performs on the holdout data and compare the value predicted by model 5 to the actual values. The graph suggests that the model is able to capture the seasonal variation in the data, thus we can conclude that that the patterns in the data are stable. 

```{r, echo=FALSE }
g_predictions
```

## 5. Conclusion 

The case study was aimed to predict the volume of ticket sales for open air pools in Albuquerque. The used data included transaction level data. The chosen model (based on CV RMSE) included variables like linear trend and different dummies, indicating seasonality. It was able to capture the seasonal variation in the holdout and training set, however, the model tends to make errors in months where there are longer holidays, such as December. In order to improve the model performance, it would be beneficial to new variables, like the weather conditions or dummy variable to indicate if it was cold on a given day. In addition, using a different prediction algorithm such as Prophet would have the potentila to improve prediction, however due to technical reasons the package could not be used for the current case study. 
